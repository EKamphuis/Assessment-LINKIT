#My experience and further notes

## Starting the Assessment
The first thing I did is to look up the different programs to see how they work, but I didn't really understand all the explanations. Mainly because there was way too much information and not always that understandable if you have no experience. Thus I was a bit lost in the load of information and my resolution was to just start with it and as I took each step to look up the information that i needed to perform this step and the next one. If I run into an error, just google it and try to solve it. Didn't that work than forget about it and try to do it in a different way. That was mainly the approach I had during this assessment and it that worked in most cases.

## HDP Sandbox
The first thing I did was trying to download the HDP sandbox. 
I had quite some problems in starting de HDP Sandbox. First I tried to use it through Virtual box, but that didn't work. Most likely because my computer doesn't have enough RAM capacity. THan i tried it through Microsoft Azure. I did get it downloaded, but couldn't perform any coding into HIVE or Zeppelin. Still not sure why. Most likely the programs did start, but the loading times where too long. Than after contacting Boy and Thiago I decided to stop trying to get HDP Sandbox to work and look for a different solution. I found a website that very clearly expalained how you can use Spark and Hbase togheter in a Microsoft Azure account by downloading both programs using HDInsight (https://docs.microsoft.com/en-us/azure/hdinsight/hdinsight-using-spark-query-hbase). Thus that was what I first tried. Downloading both programs wasn't an issue. I did figure out during the downloading process that I also needed a storage box and a virtual network to run both programs. I installed a new storagebox and I used the HDP Sandbox virtual network to start both programs. I wasn't sure if that were the smartest ways to do it. But I assumed if it was a problem I would run into it the moment I wanted to use the programs. I did run into a lot of errors during this process. Which included the settings of my own laptop and the settings in the microsoft azure account. But googling the errors resulted in a solution in most cases. If it didn't I would try out some different settings or skip it and try something else out. For instance I had Git Bash installed, but didn't know in the beginning how to connect with sandbox. Later I found out you don't need to use git bash, but can use the local:4200 portal. After that when I wanted to connect to my github I did need it and than tried again to understand how to use it. And I did using different youtube films. In the end I did get both programs working. When the programs are working I think that running the programs is quite straight forward. The only problem I did run into was the syntax. A lot of people use these programs of course, but a lot of them have either different versions or they installed the programs differently, which results in small changes of the syntax. Something I was also first confused by is that they were using different languages in the same screen/program. But again google did help/fix a lot.

## HDFS & Hive on Spark
When i googled using HIVE and Spark to create tables I got a lot of options. I found one website that really detailed how to create tables in HIVE. This seemed to be very simple and straight forward, thus I started by following this website. The code was very straight forward and lucky enough I do have some experience with SQL. Following the instructions I first put the csv files in my storagebox in the /tmp/ files. Because the storagebox was already linked to the program it was easy to acces it. After 'making' de different tables I started to look into Spark. I run most of the instructions paragraphs and tried to collect the information i needed to run the tables I made in HIVE. To create a table that contains driverId, name, hours_logged and miles_logged I had to join two tables. Drivers and timesheet. I did have some trouble to figure out wich language to use. Especially because you can use SQL directly on your tables (%live2.sql). But you cannot join them directly. I had to do that in spark. I also succeeded in making dataframes from the separate tables. That was simple. But I had to combine the languages to join them. I did come across a solution by using spark language including sql, but I didn't get the syntax right. After comparing 5 websites and looking into Zeppelin instruction paragraphs I did finally figure out what the solution was. I made a small error in the Syntax. After finally running the paragraph I also wanted to see the output and YES, there were only four columns. See the coding and output in the attached files. 

## HBase
The next step was to get Hbase started. After googling what the purpose is of this program, I tried to find out how to acces it. This did give me quite some problems. Because a lot of people explain only the code and not the program to use. I only new one program I had on my computer to process coding and might be able to interact with Hbase, wich was my git bash. I tried it out and found out it did work! I was able to interact with Hbase and enter the hbase shell. The first thing I tried to do was create an empty table and import the csv file dangerous-driver. I found a lot of information how to adapt tables in Hbase and I wanted, after importing the data, apply the different adaptations asked for this assignment. But than i did run into quite a problem, because whatever syntax I used it didn't work. I could create an empty table, but I didn't succeed in importing the necessairy data. A lot of information I found was from people that use Sandbox. Thus I can imagine the paths they use in the syntax are different. I also checked my local folder. I put the neccesairy files in my storage box, but this time in the Hbase folder. Assuming I could asses it from there. Because when I used Spark I could acces it. I didn't succeed in finishing this part of the assignment. I did attach a file containing the codes and potential code that i tried to execute this part of the assignment. 

## Sidenotes 
Quite late in the game i tried to figure out how to copy data or codes to github in the resperatory. Why? Because I could also do that manually and as a newby and with time pressure I assumed it was faster that way than trying to figure out how that worked. And the first thing I focused on was getting the programs to work. But later I did look into it and I could connect to github through gitbash and also cloned this resporatory. But the rest of the data I put it in manually. I am happy to know about this system. I do think it is a really nice tool to know about!

My basic approach was first trying to understand the first steps (for instance: first look up how to make the tables in HIVE), execute that and then move on to the next step. In some cases that was the right thing to do. But I also realized that I could have been more efficient if I had a better overview of all the programs. On the other hand I also wanted to keep going at executing steps instead of only reading, because I also quickly found out that people in general have a different approach. One solution might be easier to execute than other options. By trying it out I also tried to figure out what worked for me at the moment to get the tasks done. If it didn't work I movent on to the next explanation. Trial and error was basically my method, with google as my guide. And of course also reading into what I was doing, but just until the basics. I would really like to have a better understanding of what I am doing, but because of time restrictions I tried to stay away from that too much for now.

I really enjoyed my time doing this! I wish I could work on it longer, especially since I am finally starting to understand how it kinda works and also started to understand what connects the programs and how to use them. I am very thankful for this experience! I learned a lot in a very short timeframe. Even the program lingo is becoming more familiar to me.

